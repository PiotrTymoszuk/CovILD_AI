@article{Nikolaou2021,
abstract = {Background Chronic obstructive pulmonary disease (COPD) is a heterogeneous group of lung conditions challenging to diagnose and treat. Identification of phenotypes of patients with lung function loss may allow early intervention and improve disease management. We characterised patients with the ‘fast decliner' phenotype, determined its reproducibility and predicted lung function decline after COPD diagnosis.Methods A prospective 4 years observational study that applies machine learning tools to identify COPD phenotypes among 13 260 patients from the UK Royal College of General Practitioners and Surveillance Centre database. The phenotypes were identified prior to diagnosis (training data set), and their reproducibility was assessed after COPD diagnosis (validation data set).Results Three COPD phenotypes were identified, the most common of which was the ‘fast decliner'—characterised by patients of younger age with the lowest number of COPD exacerbations and better lung function—yet a fast decline in lung function with increasing number of exacerbations. The other two phenotypes were characterised by (a) patients with the highest prevalence of COPD severity and (b) patients of older age, mostly men and the highest prevalence of diabetes, cardiovascular comorbidities and hypertension. These phenotypes were reproduced in the validation data set with 80% accuracy. Gender, COPD severity and exacerbations were the most important risk factors for lung function decline in the most common phenotype.Conclusions In this study, three COPD phenotypes were identified prior to patients being diagnosed with COPD. The reproducibility of those phenotypes in a blind data set following COPD diagnosis suggests their generalisability among different populations.},
author = {Nikolaou, Vasilis and Massaro, Sebastiano and Garn, Wolfgang and Fakhimi, Masoud and Stergioulas, Lampros and Price, David B.},
doi = {10.1136/BMJRESP-2021-000980},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nikolaou et al. - 2021 - Fast decliner phenotype of chronic obstructive pulmonary disease (COPD) applying machine learning for predictin.pdf:pdf},
issn = {2052-4439},
journal = {BMJ Open Respiratory Research},
keywords = {COPD epidemiology,COPD exacerbations},
month = {oct},
number = {1},
publisher = {British Thoracic Society},
title = {{Fast decliner phenotype of chronic obstructive pulmonary disease (COPD): applying machine learning for predicting lung function loss}},
url = {https://bmjopenrespres.bmj.com/content/8/1/e000980},
volume = {8},
year = {2021}
}
@article{Murdaca2021,
abstract = {Introduction: Systemic sclerosis (SSc) is a systemic immune-mediated disease, featuring fibrosis of the skin and organs, and has the greatest mortality among rheumatic diseases. The nervous system involvement has recently been demonstrated, although actual lung involvement is considered the leading cause of death in SSc and, therefore, should be diagnosed early. Pulmonary function tests are not sensitive enough to be used for screening purposes, thus they should be flanked by other clinical examinations; however, this would lead to a risk of overtesting, with considerable costs for the health system and an unnecessary burden for the patients. To this extent, Machine Learning (ML) algorithms could represent a useful add-on to the current clinical practice for diagnostic purposes and could help retrieve the most useful exams to be carried out for diagnostic purposes. Method: Here, we retrospectively collected high resolution computed tomography, pulmonary function tests, esophageal pH impedance tests, esophageal manometry and reflux disease questionnaires of 38 patients with SSc, applying, with R, different supervised ML algorithms, including lasso, ridge, elastic net, classification and regression trees (CART) and random forest to estimate the most important predictors for pulmonary involvement from such data. Results: In terms of performance, the random forest algorithm outperformed the other classifiers, with an estimated root-mean-square error (RMSE) of 0.810. However, this algorithm was seen to be computationally intensive, leaving room for the usefulness of other classifiers when a shorter response time is needed. Conclusions: Despite the notably small sample size, that could have prevented obtaining fully reliable data, the powerful tools available for ML can be useful for predicting early lung involvement in SSc patients. The use of predictors coming from spirometry and pH impedentiometry together might perform optimally for predicting early lung involvement in SSc.},
author = {Murdaca, Giuseppe and Caprioli, Simone and Tonacci, Alessandro and Billeci, Lucia and Greco, Monica and Negrini, Simone and Cittadini, Giuseppe and Zentilin, Patrizia and Spagnolo, Elvira Ventura and Gangemi, Sebastiano},
doi = {10.3390/DIAGNOSTICS11101880},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdaca et al. - 2021 - A Machine Learning Application to Predict Early Lung Involvement in Scleroderma A Feasibility Evaluation.pdf:pdf},
issn = {20754418},
journal = {Diagnostics},
keywords = {Artificial intelligence,Esophageal dilatation,HRCT chest,Machine learning,Systemic sclerosis},
month = {oct},
number = {10},
pages = {1880},
pmid = {34679580},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
title = {{A Machine Learning Application to Predict Early Lung Involvement in Scleroderma: A Feasibility Evaluation}},
url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC8534403/},
volume = {11},
year = {2021}
}
@article{Sharifi2020,
abstract = {Background: Pulmonary complications, including infections, are highly prevalent in patients after hematopoietic cell transplantation with chronic graft-vs-host disease. These comorbid diseases can make the diagnosis of early lung graft-vs-host disease (bronchiolitis obliterans syndrome) challenging. A quantitative method to differentiate among these pulmonary diseases can address diagnostic challenges and facilitate earlier and more targeted therapy. Study Design and Methods: We conducted a single-center study of 66 patients with CT chest scans analyzed with a quantitative imaging tool known as parametric response mapping. Parametric response mapping results were correlated with pulmonary function tests and clinical characteristics. Five parametric response mapping metrics were applied to K-means clustering and support vector machine models to distinguish among posttransplantation lung complications solely from quantitative output. Results: Compared with parametric response mapping, spirometry showed a moderate correlation with radiographic air trapping, and total lung capacity and residual volume showed a strong correlation with radiographic lung volumes. K-means clustering analysis distinguished four unique clusters. Clusters 2 and 3 represented obstructive physiology (encompassing 81% of patients with bronchiolitis obliterans syndrome) in increasing severity (percentage air trapping 15.6% and 43.0%, respectively). Cluster 1 was dominated by normal lung, and cluster 4 was characterized by patients with parenchymal opacities. A support vector machine algorithm differentiated bronchiolitis obliterans syndrome with a specificity of 88%, sensitivity of 83%, accuracy of 86%, and an area under the receiver operating characteristic curve of 0.85. Interpretation: Our machine learning models offer a quantitative approach for the identification of bronchiolitis obliterans syndrome vs other lung diseases, including late pulmonary complications after hematopoietic cell transplantation.},
author = {Sharifi, Husham and Lai, Yu Kuang and Guo, Henry and Hoppenfeld, Mita and Guenther, Zachary D. and Johnston, Laura and Brondstetter, Theresa and Chhatwani, Laveena and Nicolls, Mark R. and Hsu, Joe L.},
doi = {10.1016/J.CHEST.2020.02.076},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sharifi et al. - 2020 - Machine Learning Algorithms to Differentiate Among Pulmonary Complications After Hematopoietic Cell Transplant.pdf:pdf},
issn = {19313543},
journal = {Chest},
keywords = {bone marrow,bronchiolitis obliterans,graft vs host,medical informatics,organizing pneumonia,radiology—thoracic},
month = {sep},
number = {3},
pages = {1090},
pmid = {32343962},
publisher = {Elsevier Inc},
title = {{Machine Learning Algorithms to Differentiate Among Pulmonary Complications After Hematopoietic Cell Transplant}},
url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC8097633/},
volume = {158},
year = {2020}
}
@article{Park2023,
abstract = {Background: Low-dose chest CT screening is recommended for smokers with the potential for lung function abnormality, but its role in predicting lung function remains unclear. Purpose: To develop a deep learning algorithm to predict pulmonary function with low-dose CT images in participants using health screening services. Materials and Methods: In this retrospective study, participants underwent health screening with same-day low-dose CT and pulmonary function testing with spirometry at a university affiliated tertiary referral general hospital between January 2015 and December 2018. The data set was split into a development set (model training, validation, and internal test sets) and temporally independent test set according to first visit year. A convolutional neural network was trained to predict the forced expiratory volume in the first second of expiration (FEV1) and forced vital capacity (FVC) from low-dose CT. The mean absolute error and concordance correlation coefficient (CCC) were used to evaluate agreement between spirometry as the reference standard and deep-learning prediction as the index test. FVC and FEV1 percent predicted (hereafter, FVC% and FEV1%) values less than 80% and percent of FVC exhaled in first second (hereafter, FEV1/FVC) less than 70% were used to classify participants at high risk. Results: A total of 16 148 participants were included (mean age, 55 years ± 10 [SD]; 10 981 men) and divided into a development set (n = 13 428) and temporally independent test set (n = 2720). In the temporally independent test set, the mean absolute error and CCC were 0.22 L and 0.94, respectively, for FVC and 0.22 L and 0.91 for FEV1. For the prediction of the respiratory high-risk group, FVC%, FEV1%, and FEV1/FVC had respective accuracies of 89.6% (2436 of 2720 participants; 95% CI: 88.4, 90.7), 85.9% (2337 of 2720 participants; 95% CI: 84.6, 87.2), and 90.2% (2453 of 2720 participants; 95% CI: 89.1, 91.3) in the same testing data set. The sensitivities were 61.6% (242 of 393 participants; 95% CI: 59.7, 63.4), 46.9% (226 of 482 participants; 95% CI: 45.0, 48.8), and 36.1% (91 of 252 participants; 95% CI: 34.3, 37.9), respectively. Conclusion: A deep learning model applied to volumetric chest CT predicted pulmonary function with relatively good performance.},
author = {Park, Hyunjung and Yun, Jihye and Lee, Sang Min and Hwang, Hye Jeon and Seo, Joon Beom and Jung, Young Ju and Hwang, Jeongeun and Lee, Se Hee and Lee, Sei Won and Kim, Namkug},
doi = {10.1148/RADIOL.221488/ASSET/IMAGES/LARGE/RADIOL.221488.FIG6.JPEG},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - 2023 - Deep Learning–based Approach to Predict Pulmonary Function at Chest CT.pdf:pdf},
issn = {15271315},
journal = {Radiology},
month = {apr},
number = {2},
pmid = {36786699},
publisher = {Radiological Society of North America Inc.},
title = {{Deep Learning–based Approach to Predict Pulmonary Function at Chest CT}},
url = {https://pubs.rsna.org/doi/10.1148/radiol.221488},
volume = {307},
year = {2023}
}
@article{Carvalho2023,
abstract = {Background Coronavirus disease (COVID-19) survivors exhibit multisystemic alterations after hospitalization. Little is known about long-term imaging and pulmonary function of hospitalized patients intensive care unit (ICU) who survive COVID-19. We aimed to investigate long-term consequences of COVID-19 on the respiratory system of patients discharged from hospital ICU and identify risk factors associated with chest computed tomography (CT) lesion severity.   Methods A prospective cohort study of COVID-19 patients admitted to a tertiary hospital ICU in Brazil (March-August/2020), and followed-up six-twelve months after hospital admission. Initial assessment included: modified Medical Research Council dyspnea scale, SpO2 evaluation, forced vital capacity, and chest X-Ray. Patients with alterations in at least one of these examinations were eligible for CT and pulmonary function tests (PFTs) approximately 16 months after hospital admission. Primary outcome: CT lesion severity (fibrotic-like or non-fibrotic-like). Baseline clinical variables were used to build a machine learning model (ML) to predict the severity of CT lesion.   Results In total, 326 patients (72%) were eligible for CT and PFTs. COVID-19 CT lesions were identified in 81.8% of patients, and half of them showed mild restrictive lung impairment and impaired lung diffusion capacity. Patients with COVID-19 CT findings were stratified into two categories of lesion severity: non-fibrotic-like (50.8%-ground-glass opacities/reticulations) and fibrotic-like (49.2%-traction bronchiectasis/architectural distortion). No association between CT feature severity and altered lung diffusion or functional restrictive/obstructive patterns was found. The ML detected that male sex, ICU and invasive mechanic ventilation (IMV) period, tracheostomy and vasoactive drug need during hospitalization were predictors of CT lesion severity(sensitivity,0.78±0.02;specificity,0.79±0.01;F1-score,0.78±0.02;positive predictive rate,0.78±0.02; accuracy,0.78±0.02; and area under the curve,0.83±0.01).   Conclusion ICU hospitalization due to COVID-19 led to respiratory system alterations six-twelve months after hospital admission. Male sex and critical disease acute phase, characterized by a longer ICU and IMV period, and need for tracheostomy and vasoactive drugs, were risk factors for severe CT lesions six-twelve months after hospital admission.},
author = {Carvalho, Carlos Roberto Ribeiro and Lamas, Celina Almeida and Chate, Rodrigo Caruso and Salge, Jo{\~{a}}o Marcos and Sawamura, Marcio Valente Yamada and de Albuquerque, Andr{\'{e}} L.P. and {Toufen Junior}, Carlos and Lima, Daniel Mario and Garcia, Michelle Louvaes and Scudeller, Paula Gobi and Nomura, Cesar Higa and Gutierrez, Marco Antonio and Baldi, Bruno Guedes},
doi = {10.1371/JOURNAL.PONE.0280567},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carvalho et al. - 2023 - Long-term respiratory follow-up of ICU hospitalized COVID-19 patients Prospective cohort study.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {COVID 19,Chronic obstructive pulmonary disease,Computed axial tomography,Hospitals,Intensive care units,Lesions,Oxygen,Tracheostomy},
month = {jan},
number = {1},
pages = {e0280567},
pmid = {36662879},
publisher = {Public Library of Science},
title = {{Long-term respiratory follow-up of ICU hospitalized COVID-19 patients: Prospective cohort study}},
url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280567},
volume = {18},
year = {2023}
}
@article{Cordelli2024,
abstract = {Long COVID is a multi-systemic disease characterized by the persistence or occurrence of many symptoms that in many cases affect the pulmonary system. These, in turn, may deteriorate the patient's quality of life making it easier to develop severe complications. Being able to predict this syndrome is therefore important as this enables early treatment. In this work, we investigated three machine learning approaches that use clinical data collected at the time of hospitalization to this goal. The first works with all the descriptors feeding a traditional shallow learner, the second exploits the benefits of an ensemble of classifiers, and the third is driven by the intrinsic multimodality of the data so that different models learn complementary information. The experiments on a new cohort of data from 152 patients show that it is possible to predict pulmonary Long Covid sequelae with an accuracy of up to $$94\%$$ . As a further contribution, this work also publicly discloses the related data repository to foster research in this field.},
author = {Cordelli, Ermanno and Soda, Paolo and Citter, Sara and Schiavon, Elia and Salvatore, Christian and Fazzini, Deborah and Clementi, Greta and Cellina, Michaela and Cozzi, Andrea and Bortolotto, Chandra and Preda, Lorenzo and Francini, Luisa and Tortora, Matteo and Castiglioni, Isabella and Papa, Sergio and Sona, Diego and Al{\`{i}}, Marco},
doi = {10.1186/S12911-024-02745-3},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cordelli et al. - 2024 - Machine learning predicts pulmonary Long Covid sequelae using clinical data.pdf:pdf},
issn = {1472-6947},
journal = {BMC Medical Informatics and Decision Making 2024 24:1},
keywords = {Health Informatics,Information Systems and Communication Service,Management of Computing and Information Systems,Post-COVID syndrome,Prognosis,artificial intelligence,long-covid,multimodal learning,post-covid syndrome,prognosis},
month = {nov},
number = {1},
pages = {1--14},
publisher = {BioMed Central},
title = {{Machine learning predicts pulmonary Long Covid sequelae using clinical data}},
url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-024-02745-3},
volume = {24},
year = {2024}
}
@article{Zhang2022a,
abstract = {Background: Data on the long-term trajectories of lung function are scarce in COVID-19 survivors. Methods: We re-analyzed the data from a prospective longitudinal cohort follow-up study of COVID-19 survivors over 2 years after infection. All participants were divided into scale 3, scale 4 and scale 5-6 groups according to seven-category ordinal scale. The changes of pulmonary function tests (PFTs), the Modified Medical Research Council (mMRC) Dyspnea Scale, 6-min walking test health-related quality of life (HRQoL) across the three serial follow-up visits were evaluated, and compared among three groups. We performed liner regression to determine potential factors that were associated with changes of PFTs and distance walked in 6 minutes (6MWD). Findings: In this study, 288 participants generally presented an improvement of PFTs parameters from 6 months to 1 year after infection. The scale 5-6 group displayed a significantly higher increase of PFTs compared with scale 3 and scale 4 groups (all p<0.0167), and corticosteroids therapy was identified as a protective factor for the PFTs improvement with a correlation coefficient of 2.730 (0.215–5.246) for forced vital capacity (FVC), 2.909 (0.383–5.436) for total lung capacity (TLC), and 3.299 (0.211–6.387) for diffusion capacity for carbon monoxide (DLco), respectively. From 1-year to 2-year follow-up, the PFTs parameters generally decreased, which was not observed to be associated with changes of 6MWD and HRQoL. Dyspnea (mMRC≥1) generally decreased over time (23.3% [61/262] for 6-month, 27.9% [67/240] for 1-year, 13.4% [35/261] for 2-year), and 6MWD increased continuously (500.0 m vs 505.0 m vs 525.0 m). Interpretation: Corticosteroids therapy during hospitalization was a protective factor for PFTs improvement from 6 months to 1 year. The relatively fast decline trend of PFTs from 1 year to 2 years needs to be paid attention and further validated in the future follow-up study. Fundings: This work was supported by Chinese Academy of Medical Sciences Innovation Fund for Medical Sciences (CIFMS 2021-I2M-1-048) and the National Key Research and Development Program of China (2021YFC0864700).},
author = {Zhang, Hui and Li, Xia and Huang, Lixue and Gu, Xiaoyin and Wang, Yimin and Liu, Min and Liu, Zhibo and Zhang, Xueyang and Yu, Zhenxing and Wang, Yeming and Huang, Chaolin and Cao, Bin},
doi = {10.1016/J.ECLINM.2022.101668},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2022 - Lung-function trajectories in COVID-19 survivors after discharge A two-year longitudinal cohort study.pdf:pdf},
issn = {25895370},
journal = {eClinicalMedicine},
keywords = {COVID-19,Corticosteroids,Long COVID,Lung function,Pulmonary function tests},
month = {dec},
pages = {101668},
pmid = {36188433},
publisher = {Elsevier Ltd},
title = {{Lung-function trajectories in COVID-19 survivors after discharge: A two-year longitudinal cohort study}},
url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC9514976/},
volume = {54},
year = {2022}
}
@article{Zhang2005,
abstract = {Gene co-expression networks are increasingly used to explore the system-level functionality of genes. The network construction is conceptually straightforward: nodes represent genes and nodes are connected if the corresponding genes are significantly co-expressed across appropriately chosen tissue samples. In reality, it is tricky to define the connections between the nodes in such networks. An important question is whether it is biologically meaningful to encode gene co-expression using binary information (connected=1, unconnected=0). We describe a general framework for 'soft' thresholding that assigns a connection weight to each gene pair. This leads us to define the notion of a weighted gene co-expression network. For soft thresholding we propose several adjacency functions that convert the co-expression measure to a connection weight. For determining the parameters of the adjacency function, we propose a biologically motivated criterion (referred to as the scale-free topology criterion). We generalize the following important network concepts to the case of weighted networks. First, we introduce several node connectivity measures and provide empirical evidence that they can be important for predicting the biological significance of a gene. Second, we provide theoretical and empirical evidence that the 'weighted' topological overlap measure (used to define gene modules) leads to more cohesive modules than its 'unweighted' counterpart. Third, we generalize the clustering coefficient to weighted networks. Unlike the unweighted clustering coefficient, the weighted clustering coefficient is not inversely related to the connectivity. We provide a model that shows how an inverse relationship between clustering coefficient and connectivity arises from hard thresholding. We apply our methods to simulated data, a cancer microarray data set, and a yeast microarray data set. Copyright {\textcopyright}2005 by the authors. All rights reserved.},
author = {Zhang, Bin and Horvath, Steve},
doi = {10.2202/1544-6115.1128/MACHINEREADABLECITATION/RIS},
issn = {15446115},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {Clustering coefficient,Hierarchical organization,Microarrays,Module,Network analysis,Scale-free topology,Topological overlap},
month = {aug},
number = {1},
pmid = {16646834},
publisher = {Walter de Gruyter GmbH},
title = {{A general framework for weighted gene co-expression network analysis}},
url = {https://www.degruyter.com/document/doi/10.2202/1544-6115.1128/html},
volume = {4},
year = {2005}
}
@article{Viering2023,
abstract = {Learning curves provide insight into the dependence of a learner's generalization performance on the training set size. This important tool can be used for model selection, to predict the effect of more training data, and to reduce the computational complexity of model training and hyperparameter tuning. This review recounts the origins of the term, provides a formal definition of the learning curve, and briefly covers basics such as its estimation. Our main contribution is a comprehensive overview of the literature regarding the shape of learning curves. We discuss empirical and theoretical evidence that supports well-behaved curves that often have the shape of a power law or an exponential. We consider the learning curves of Gaussian processes, the complex shapes they can display, and the factors influencing them. We draw specific attention to examples of learning curves that are ill-behaved, showing worse learning performance with more training data. To wrap up, we point out various open problems that warrant deeper empirical and theoretical investigation. All in all, our review underscores that learning curves are surprisingly diverse and no universal model can be identified.},
archivePrefix = {arXiv},
arxivId = {2103.10948},
author = {Viering, Tom and Loog, Marco},
doi = {10.1109/TPAMI.2022.3220744},
eprint = {2103.10948},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {MEDLINE,Marco Loog,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Tom Viering,doi:10.1109/TPAMI.2022.3220744,pmid:36350870},
month = {jun},
number = {6},
pages = {7799--7819},
pmid = {36350870},
publisher = {IEEE Trans Pattern Anal Mach Intell},
title = {{The Shape of Learning Curves: A Review}},
url = {https://pubmed.ncbi.nlm.nih.gov/36350870/},
volume = {45},
year = {2023}
}
@article{Chawla2002,
abstract = {An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    ``normal'' examples with only a small percentage of ``abnormal'' or    ``interesting'' examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/JAIR.953},
eprint = {1106.1813},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {jun},
pages = {321--357},
publisher = {American Association for Artificial Intelligence},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
url = {https://www.jair.org/index.php/jair/article/view/10302},
volume = {16},
year = {2002}
}
@misc{Wood2023,
abstract = {Generalized additive (mixed) models, some of their extensions and other generalized ridge regression with multiple smoothing parameter estimation by (Restricted) Marginal Likelihood, Generalized Cross Validation and similar, or using iterated nested Laplace approximation for fully Bayesian inference. See Wood (2017) <doi:10.1201/9781315370279> for an overview. Includes a gam() function, a wide variety of smoothers, 'JAGS' support and distributions beyond the exponential family.},
author = {Wood, Simon},
title = {{mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation}},
url = {https://cran.r-project.org/web/packages/mgcv/index.html},
year = {2023}
}
@misc{Mayer2023,
abstract = {Visualizations for SHAP (SHapley Additive exPlanations), such as waterfall plots, force plots, various types of importance plots, dependence plots, and interaction plots. These plots act on a 'shapviz' object created from a matrix of SHAP values and a corresponding feature dataset. Wrappers for the R packages 'xgboost', 'lightgbm', 'fastshap', 'shapr', 'h2o', 'treeshap', 'DALEX', and 'kernelshap' are added for convenience. By separating visualization and computation, it is possible to display factor variables in graphs, even if the SHAP values are calculated by a model that requires numerical features. The plots are inspired by those provided by the 'shap' package in Python, but there is no dependency on it.},
author = {Mayer, Michael and Stando, Adrian},
title = {{shapviz: SHAP Visualizations}},
url = {https://cran.r-project.org/web/packages/shapviz/index.html},
year = {2023}
}
@misc{Mayer2023a,
abstract = {Efficient implementation of Kernel SHAP, see Lundberg and Lee (2017) <https://dl.acm.org/doi/10.5555/3295222.3295230>, and Covert and Lee (2021) <http://proceedings.mlr.press/v130/covert21a>. For models with up to eight features, the results are exact regarding the selected background data. Otherwise, an almost exact hybrid algorithm involving iterative sampling is used. The package plays well together with meta-learning packages like 'tidymodels', 'caret' or 'mlr3'. Visualizations can be done using the R package 'shapviz'.},
author = {Mayer, Michael and Watson, David and Biecek, Przemyslaw},
title = {{kernelshap: Kernel SHAP}},
url = {https://cran.r-project.org/web/packages/kernelshap/index.html},
year = {2023}
}
@article{Covert2020,
abstract = {The Shapley value concept from cooperative game theory has become a popular
technique for interpreting ML models, but efficiently estimating these values
remains challenging, particularly in the model-agnostic setting. Here, we
revisit the idea of estimating Shapley values via linear regression to
understand and improve upon this approach. By analyzing the original KernelSHAP
alongside a newly proposed unbiased version, we develop techniques to detect
its convergence and calculate uncertainty estimates. We also find that the
original version incurs a negligible increase in bias in exchange for
significantly lower variance, and we propose a variance reduction technique
that further accelerates the convergence of both estimators. Finally, we
develop a version of KernelSHAP for stochastic cooperative games that yields
fast new estimators for two global explanation methods.},
archivePrefix = {arXiv},
arxivId = {2012.01536},
author = {Covert, Ian and Lee, Su In},
eprint = {2012.01536},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Covert, Lee - 2020 - Improving KernelSHAP Practical Shapley Value Estimation via Linear Regression.pdf:pdf},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
month = {dec},
pages = {3457--3465},
publisher = {ML Research Press},
title = {{Improving KernelSHAP: Practical Shapley Value Estimation via Linear Regression}},
url = {https://arxiv.org/abs/2012.01536v3},
volume = {130},
year = {2020}
}
@article{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M. and Lee, Su In},
eprint = {1705.07874},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Lee - 2017 - A Unified Approach to Interpreting Model Predictions.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {may},
pages = {4766--4775},
publisher = {Neural information processing systems foundation},
title = {{A Unified Approach to Interpreting Model Predictions}},
url = {https://arxiv.org/abs/1705.07874v2},
volume = {2017-Decem},
year = {2017}
}
@misc{Slowikowski2023,
abstract = {Provides text and label geoms for 'ggplot2' that help to avoid overlapping text labels. Labels repel away from each other and away from the data points.},
author = {Slowikowski, Kamil and Shep, Alicia and Hughes, Sean and Dang, Trung Kien and Lukauskas, Saulius and Irisson, Jean-Olivier and Kamvar, Zhian N and Ryan, Thompson and Dervieux, Christophe and Yutani, Hiroaki and Gramme, Pierre and Abdol, Amir Masoud and Barrett, Malcolm and Cannoodt, Robrecht and Krassowski, Micha{\l} and Chirico, Michael and Aphalo, Perdo},
title = {{ggrepel: Automatically Position Non-Overlapping Text Labels with 'ggplot2'}},
url = {https://cran.r-project.org/web/packages/ggrepel/index.html},
year = {2023}
}
@misc{Folashade2022,
abstract = {Provides a parallel backend for the %dopar% function using the parallel package.},
author = {Folashade, Daniel and {Microsoft Corporation} and Weston, Steve and Tenenbaum, Dan},
title = {{doParallel: Foreach Parallel Adaptor for the 'parallel' Package}},
url = {https://cran.r-project.org/web/packages/doParallel/index.html},
year = {2022}
}
@article{Natekin2013,
abstract = {Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed. {\textcopyright} 2013 Natekin and Knoll.},
author = {Natekin, Alexey and Knoll, Alois},
doi = {10.3389/FNBOT.2013.00021/BIBTEX},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Natekin, Knoll - 2013 - Gradient boosting machines, a tutorial.pdf:pdf},
issn = {16625218},
journal = {Frontiers in Neurorobotics},
keywords = {Boosting,Classification,Gradient boosting,Machine learning,Regression,Robotic control,Text classification},
month = {dec},
number = {DEC},
pages = {63623},
publisher = {Frontiers Research Foundation},
title = {{Gradient boosting machines, a tutorial}},
volume = {7},
year = {2013}
}
@misc{Greenwell2022,
abstract = {An implementation of extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine. Includes regression methods for least squares, absolute loss, t-distribution loss, quantile regression, logistic, multinomial logistic, Poisson, Cox proportional hazards partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway.},
author = {Greenwell, Brandon and Boehmke, Bradley and Cunningham, Jay and Developers, GBM},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{gbm: Generalized Boosted Regression Models}},
url = {https://cran.r-project.org/package=gbm},
year = {2022}
}
@article{Friedman2002,
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current "pseudo'-residuals by least squares at each iteration. The ...},
author = {Friedman, Jerome H.},
doi = {10.1016/S0167-9473(01)00065-2},
issn = {01679473},
journal = {Computational Statistics & Data Analysis},
month = {feb},
number = {4},
pages = {367--378},
publisher = {
		Elsevier Science Publishers B. V.
		PUB568
		Amsterdam, The Netherlands, The Netherlands
	},
title = {{Stochastic gradient boosting}},
url = {https://dl.acm.org/doi/10.1016/S0167-9473%2801%2900065-2},
volume = {38},
year = {2002}
}
@article{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
author = {Friedman, Jerome H.},
doi = {10.1214/AOS/1013203451},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman - 2001 - Greedy function approximation A gradient boosting machine.pdf:pdf},
issn = {0090-5364},
journal = {https://doi.org/10.1214/aos/1013203451},
keywords = {62-02,62-07,62-08,62G08,62H30,68T10,Function estimation,boosting,decision trees,robust nonparametric regression},
month = {oct},
number = {5},
pages = {1189--1232},
publisher = {Institute of Mathematical Statistics},
title = {{Greedy function approximation: A gradient boosting machine.}},
url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-appro},
volume = {29},
year = {2001}
}
@misc{Goldstein-Greenwood2021,
author = {Goldstein-Greenwood, Jacob},
title = {{A Brief on Brier Scores | UVA Library}},
url = {https://library.virginia.edu/data/articles/a-brief-on-brier-scores},
urldate = {2023-09-05},
year = {2021}
}
@article{Hopkins1954,
abstract = {The method depends on linear measurements between random points and adjacent individuals, and between adjacent pairs of individuals. Its results compare favourably with those of the current methods when tested on synthetic and natural populations. The method is quicker than the quadrat methods and is especially useful for analysing the distribution of trees. {\textcopyright} 1954 Oxford University Press.},
author = {Hopkins, Brian and Skellam, J. G.},
doi = {10.1093/OXFORDJOURNALS.AOB.A083391},
issn = {0305-7364},
journal = {Annals of Botany},
month = {apr},
number = {2},
pages = {213--227},
publisher = {Oxford Academic},
title = {{A New Method for determining the Type of Distribution of Plant Individuals}},
url = {https://dx.doi.org/10.1093/oxfordjournals.aob.a083391},
volume = {18},
year = {1954}
}
@article{Rice2005,
abstract = {In order to facilitate comparisons across follow-up studies that have used different measures of effect size, we provide a table of effect size equivalencies for the three most common measures: ROC area (AUC), Cohen's d, and r. We outline why AUC is the preferred measure of predictive or diagnostic accuracy in forensic psychology or psychiatry, and we urge researchers and practitioners to use numbers rather than verbal labels to characterize effect sizes. {\textcopyright} 2005 American Psychology-Law Society/Division 41 of the American Psychological Association.},
author = {Rice, Marnie E. and Harris, Grant T.},
doi = {10.1007/S10979-005-6832-7},
issn = {01477307},
journal = {Law and Human Behavior},
keywords = {Effect size,Predictive accuracy,ROC area,Risk assessment},
month = {oct},
number = {5},
pages = {615--620},
pmid = {16254746},
title = {{Comparing effect sizes in follow-up studies: ROC area, Cohen's d, and r}},
volume = {29},
year = {2005}
}
@article{Breiman2017,
abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
doi = {10.1201/9781315139470/CLASSIFICATION-REGRESSION-TREES-LEO-BREIMAN},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman et al. - 2017 - Classification and regression trees.pdf:pdf},
isbn = {9781351460491},
journal = {Classification and Regression Trees},
month = {jan},
pages = {1--358},
publisher = {CRC Press},
title = {{Classification and regression trees}},
url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman},
year = {2017}
}
@article{Garson1991,
author = {Garson, G. David},
doi = {10.5555/129449.129452},
journal = {AI Expert},
number = {4},
pages = {47 --51},
title = {{Interpreting neural-network connection weights}},
url = {https://www.scinapse.io/papers/1833005471},
volume = {6},
year = {1991}
}
@article{Cohen1960,
abstract = {A coefficient of interjudge agreement for nominal scales, formula-omitted, is presented. It is directly interpretable as the pro-portion of joint judgments in which there is agreement, after chance agreement is excluded. Its upper limit is +1.00, and its lower limit falls between zero and -1.00, depending on the distribution of judgments by the two judges. The maximum value which x can take for any given problem is given, and the implications of this value to the question of agreement discussed. An interesting characteristic of x is its identity with 0 in the dichotomous case when the judges give the same marginal distributions. Finally, its standard error and techniques for estimation and hypothesis testing are presented. {\textcopyright} 1960, Sage Publications. All rights reserved.},
author = {Cohen, Jacob},
doi = {10.1177/001316446002000104},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {1},
pages = {37--46},
title = {{A Coefficient of Agreement for Nominal Scales}},
volume = {20},
year = {1960}
}
@article{Brier1950,
abstract = {Two methods of solving the balance equation are outlined. Both methods have been used successfully on a daily operational basis at the Joint Numerical Weather Prediction Unit for a period of more than a year. Solutions were on the operational grid of 30 x 34 points spaced at 381-km. intervals.},
author = {Brier, Glenn W.},
doi = {10.1175/1520-0493(1950)078<0001:vofeit>2.0.co;2},
issn = {0027-0644},
journal = {Monthly Weather Review},
number = {1},
pages = {1--3},
title = {{VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY}},
url = {https://ui.adsabs.harvard.edu/abs/1950MWRv...78....1B/abstract},
volume = {78},
year = {1950}
}
@misc{Therneau2022,
abstract = {Recursive partitioning for classification, regression and survival trees. An implementation of most of the functionality of the 1984 book by Breiman, Friedman, Olshen and Stone.},
author = {Therneau, Terry M. and Atkinson, Beth and Ripley, Brian D.},
title = {{rpart: Recursive Partitioning and Regression Trees}},
url = {https://cran.r-project.org/web/packages/rpart/index.html},
year = {2022}
}
@article{Karatzoglou2004,
abstract = {kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.},
author = {Karatzoglou, Alexandros and Hornik, Kurt and Smola, Alex and Zeileis, Achim},
doi = {10.18637/JSS.V011.I09},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karatzoglou et al. - 2004 - kernlab - An S4 Package for Kernel Methods in R.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {Clustering,Kernel methods,Quadratic programming,R,Ranking,S4,Support vector machines},
month = {nov},
pages = {1--20},
publisher = {American Statistical Association},
title = {{kernlab - An S4 Package for Kernel Methods in R}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v011i09},
volume = {11},
year = {2004}
}
@article{Hufner2023,
author = {H{\"{u}}fner, Katharina and Tymoszuk, Piotr and Sahanic, Sabina and Luger, Anna and Boehm, Anna and Pizzini, Alex and Schwabl, Christoph and Koppelst{\"{a}}tter, Sabine and Kurz, Katharina and Asshoff, Malte and Mosheimer-Feistritzer, Birgit and Pfeifer, Bernhard and Rass, Verena and Schroll, Andrea and Iglseder, Sarah and Egger, Alexander and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Helbok, Raimund and Widmann, Gerlig and Sonnweber, Thomas and Tancevski, Ivan and Sperner-Unterweger, Barbara and L{\"{o}}ffler-Ragg, Judith},
doi = {10.1016/J.JPSYCHORES.2023.111234},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{u}}fner et al. - 2023 - Persistent somatic symptoms are key to individual illness perception at one year after COVID-19 in a cross-sectio.pdf:pdf},
issn = {0022-3999},
journal = {Journal of Psychosomatic Research},
month = {jun},
pages = {111234},
publisher = {Elsevier},
title = {{Persistent somatic symptoms are key to individual illness perception at one year after COVID-19 in a cross-sectional analysis of a prospective cohort study}},
volume = {169},
year = {2023}
}
@article{Cohen2013,
abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
author = {Cohen, Jacob},
doi = {10.4324/9780203771587},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen - 2013 - Statistical Power Analysis for the Behavioral Sciences.pdf:pdf},
isbn = {9780203771587},
journal = {Statistical Power Analysis for the Behavioral Sciences},
month = {may},
publisher = {Routledge},
title = {{Statistical Power Analysis for the Behavioral Sciences}},
url = {https://www.taylorfrancis.com/books/mono/10.4324/9780203771587/statistical-power-analysis-behavioral-sciences-jacob-cohen},
year = {2013}
}
@article{Huber2011,
abstract = {IntroductionThe term ” robust” was introduced into the statistical literature by Box (1953). By then, robust methods such as trimmed means, had been in sporadic use for well over a century, see for example Anonymous (1821). However, Tukey (1960) was the first person to recognize the extreme sensitivity of some conventional statistical procedures to seemingly minor deviations from the assumptions, and to give an eye-opening example. His example, and his realization that statistical methods optimized for the conventional Gaussian model are unstable under small perturbations were crucial for the subsequent theoretical developments initiated by Huber (1964) and Hampel (1968).In the 1960s robust methods still were considered ” dirty” by most. Therefore, to promote their reception in the statistical community it was crucial to mathematize the approach: one had to prove optimality properties, as was done by Huber's minimax results (1964, 1965, 1968), and to give a formal definition ...},
author = {Huber, Peter J.},
doi = {10.1007/978-3-642-04898-2_594},
journal = {International Encyclopedia of Statistical Science},
pages = {1248--1251},
publisher = {Springer, Berlin, Heidelberg},
title = {{Robust Statistics}},
url = {https://link.springer.com/referenceworkentry/10.1007/978-3-642-04898-2_594},
year = {2011}
}
@article{Sahanic2023,
author = {Sahanic, Sabina and Tymoszuk, Piotr and Luger, Anna K. and H{\"{u}}fner, Katharina and Boehm, Anna and Pizzini, Alex and Schwabl, Christoph and Koppelst{\"{a}}tter, Sabine and Kurz, Katharina and Asshoff, Malte and Mosheimer-Feistritzer, Birgit and Coen, Maximilian and Pfeifer, Bernhard and Rass, Verena and Egger, Alexander and H{\"{o}}rmann, Gregor and Sperner-Unterweger, Barbara and Helbok, Raimund and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Widmann, Gerlig and Tancevski, Ivan and Sonnweber, Thomas and L{\"{o}}ffler-Ragg, Judith},
doi = {10.1183/23120541.00317-2022},
issn = {2312-0541},
journal = {ERJ open research},
keywords = {Judith L{\"{o}}ffler-Ragg,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC10030059,Piotr Tymoszuk,PubMed Abstract,Sabina Sahanic,doi:10.1183/23120541.00317-2022,pmid:36960350},
month = {mar},
number = {2},
pages = {00317--2022},
pmid = {36960350},
publisher = {ERJ Open Res},
title = {{COVID-19 and its continuing burden after 12 months: a longitudinal observational prospective multicentre trial}},
url = {https://pubmed.ncbi.nlm.nih.gov/36960350/},
volume = {9},
year = {2023}
}
@article{Strobl2009,
abstract = {Recursive partitioning methods have become popular and widely used tools for nonparametric regression and classification in many scientific fields. Especially random forests, which can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine, and bioinformatics within the past few years. High-dimensional problems are common not only in genetics, but also in some areas of psychological research, where only a few subjects can be measured because of time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications and to provide descriptive variable importance measures reflecting the impact of each variable in both main effects and interactions. The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high-dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application. Application of the methods is illustrated with freely available implementations in the R system for statistical computing. {\textcopyright} 2009 American Psychological Association.},
author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
doi = {10.1037/A0016973},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobl, Malley, Tutz - 2009 - An Introduction to Recursive Partitioning Rationale, Application and Characteristics of Classification and.pdf:pdf},
issn = {1082989X},
journal = {Psychological methods},
keywords = {classification,prediction,regression,variable importance},
month = {dec},
number = {4},
pages = {323},
pmid = {19968396},
publisher = {NIH Public Access},
title = {{An Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests}},
url = {/pmc/articles/PMC2927982/ /pmc/articles/PMC2927982/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/},
volume = {14},
year = {2009}
}
@article{Hothorn2006,
abstract = {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown thai the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed. {\textcopyright} 2006 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
doi = {10.1198/106186006X133933},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Multiple testing,Multivariate regression trees,Ordinal regression trees,Permutation tests,Variable selection},
number = {3},
pages = {651--674},
publisher = {Taylor & Francis},
title = {{Unbiased recursive partitioning: A conditional inference framework}},
url = {https://www.tandfonline.com/doi/abs/10.1198/106186006X133933},
volume = {15},
year = {2006}
}
@misc{Hothorn2022,
abstract = {A computational toolbox for recursive partitioning. The core of the package is ctree(), an implementation of conditional inference trees which embed tree-structured regression models into a well defined theory of conditional inference procedures. This non-parametric class of regression trees is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Based on conditional inference trees, cforest() provides an implementation of Breiman's random forests. The function mob() implements an algorithm for recursive partitioning based on parametric models (e.g. linear models, GLMs or survival regression) employing parameter instability tests for split selection. Extensible functionality for visualizing tree-structured regression models is available. The methods are described in Hothorn et al. (2006) <doi:10.1198/106186006X133933>, Zeileis et al. (2008) <doi:10.1198/106186008X319331> and Strobl et al. (2007) <doi:10.1186/1471-2105-8-25>.},
author = {Hothorn, Thorsten and Hornik, Kurt and Strobl, Carolin and Zeileis, Achim},
title = {{party: A Laboratory for Recursive Partytioning}},
url = {https://cran.r-project.org/web/packages/party/index.html},
year = {2022}
}
@misc{Briatte2021,
abstract = {Geometries to plot network objects with 'ggplot2'.},
author = {Briatte, Fran{\c{c}}ois and Bojanowski, Michal and Canouil, Micka{\"{e}}l and Charlop-Powers, Zachary and Fisher, Jacob C. and Johnson, Kipp and Rinker, Tyler},
month = {jul},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{ggnetwork: Geometries to Plot Networks with 'ggplot2'}},
url = {https://cran.r-project.org/package=ggnetwork},
year = {2021}
}
@article{Csardi2006,
author = {Csardi, Gabor and Nepusz, Tamas},
journal = {InterJournal},
pages = {1695},
title = {{The igraph software package for complex network research}},
url = {https://igraph.org},
volume = {Complex Sy},
year = {2006}
}
@misc{Ripley2022,
abstract = {Functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S" (4th edition, 2002).},
author = {Ripley, Brian},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{MASS: Support Functions and Datasets for Venables and Ripley's MASS}},
url = {https://cran.r-project.org/package=MASS},
year = {2022}
}
@misc{Signorell2022,
abstract = {A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.},
author = {Signorell, Andri},
month = {oct},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{DescTools: Tools for Descriptive Statistics}},
url = {https://cran.r-project.org/package=DescTools},
year = {2022}
}
@misc{Vaughan2022,
abstract = {Implementations of the family of map() functions from 'purrr' that can be resolved using any 'future'-supported backend, e.g. parallel on the local machine or distributed on a compute cluster.},
author = {Vaughan, Davis and Dancho, Matt and RStudio},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{furrr: Apply Mapping Functions in Parallel using Futures}},
url = {https://cran.r-project.org/package=furrr},
year = {2022}
}
@misc{Zeileis2022,
abstract = {An S3 class with methods for totally ordered indexed observations. It is particularly aimed at irregular time series of numeric vectors/matrices and factors. zoo's key design goals are independence of a particular index/date/time class and consistency with ts and base R by providing methods to extend standard generics.},
author = {Zeileis, Achim and Grothendieck, Gabor and Ryan, Jeffrey A. and Ulrich, Joshua M. and Andrews, Felix},
month = {sep},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{zoo: S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)}},
url = {https://cran.r-project.org/package=zoo},
year = {2022}
}
@misc{Yan2021,
abstract = {An easy-to-use way to draw pretty venn diagram by 'ggplot2'.},
author = {Yan, Linlin},
month = {jun},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{ggvenn: Draw Venn Diagram by 'ggplot2'}},
url = {https://cran.r-project.org/package=ggvenn},
year = {2021}
}
@misc{Mangiafico2022,
abstract = {Functions and datasets to support "Summary and Analysis of Extension Program Evaluation in R" and "An R Companion for the Handbook of Biological Statistics". Vignettes are available at <http://rcompanion.org>.},
author = {Mangiafico, Salvatore},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{rcompanion: Functions to Support Extension Education Program Evaluation}},
url = {https://cran.r-project.org/package=rcompanion},
year = {2022}
}
@misc{Hansell2008,
abstract = {Members of the Fleischner Society compiled a glossary of terms for thoracic imaging that replaces previous glossaries published in 1984 and 1996 for thoracic radiography and computed tomography (CT), respectively. The need to update the previous versions came from the recognition that new words have emerged, others have become obsolete, and the meaning of some terms has changed. Brief descriptions of some diseases are included, and pictorial examples (chest radiographs and CT scans) are provided for the majority of terms. {\textcopyright} RSNA, 2008.},
author = {Hansell, David M. and Bankier, Alexander A. and MacMahon, Heber and McLoud, Theresa C. and M{\"{u}}ller, Nestor L. and Remy, Jacques},
booktitle = {Radiology},
doi = {10.1148/radiol.2462070712},
issn = {00338419},
month = {mar},
number = {3},
pages = {697--722},
pmid = {18195376},
publisher = {Radiological Society of North America},
title = {{Fleischner Society: Glossary of terms for thoracic imaging}},
url = {https://pubs.rsna.org/doi/10.1148/radiol.2462070712},
volume = {246},
year = {2008}
}
@misc{Gagolewski2021,
abstract = {A collection of character string/text/natural language processing tools for pattern searching (e.g., with 'Java'-like regular expressions or the 'Unicode' collation algorithm), random string generation, case mapping, string transliteration, concatenation, sorting, padding, wrapping, Unicode normalisation, date-time formatting and parsing, and many more. They are fast, consistent, convenient, and - thanks to 'ICU' (International Components for Unicode) - portable across all locales and platforms.},
author = {Gagolewski, Marek and Tartanus, Bartek},
title = {{Package 'stringi'}},
url = {https://cran.r-project.org/web/packages/stringi/index.html http://cran.ism.ac.jp/web/packages/stringi/stringi.pdf},
year = {2021}
}
@misc{Gohel2022,
abstract = {Create pretty tables for 'HTML', 'PDF', 'Microsoft Word' and 'Microsoft PowerPoint' documents from 'R Markdown'. Functions are provided to let users create tables, modify and format their content. It also extends package 'officer' that does not contain any feature for customized tabular reporting.},
author = {Gohel, David},
title = {{flextable: Functions for Tabular Reporting}},
url = {https://cran.r-project.org/web/packages/flextable/index.html},
year = {2022}
}
@misc{Xie2022,
abstract = {Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.},
author = {Xie, Yihui},
title = {{knitr: A General-Purpose Package for Dynamic Report Generation in R}},
url = {https://cran.r-project.org/web/packages/knitr/index.html},
year = {2022}
}
@misc{Allaire2022,
abstract = {Convert R Markdown documents into a variety of formats.},
author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe},
title = {{rmarkdown: Dynamic Documents for R}},
url = {https://cran.r-project.org/web/packages/rmarkdown/index.html},
year = {2022}
}
@misc{Barnier2022,
abstract = {HTML formats and templates for 'rmarkdown' documents, with some extra features such as automatic table of contents, lightboxed figures, dynamic crosstab helper.},
author = {Barnier, Julien},
title = {{rmdformats: HTML Output Formats and Templates for 'rmarkdown' Documents}},
url = {https://cran.r-project.org/web/packages/rmdformats/index.html},
year = {2022}
}
@misc{Henry2022,
abstract = {A toolbox for working with base types, core R features like the condition system, and core 'Tidyverse' features like tidy evaluation.},
author = {Henry, Lionel and Wickham, Hadley.},
title = {{rlang: Functions for Base Types and Core R and 'Tidyverse' Features}},
url = {https://cran.r-project.org/web/packages/rlang/index.html},
year = {2022}
}
@misc{Kassambara2021,
author = {Kassambara, Alboukadel},
title = {{rstatix: Pipe-Friendly Framework for Basic Statistical Tests}},
url = {https://cran.r-project.org/package=rstatix},
year = {2021}
}
@article{Luger2022,
abstract = {Background: The long-term pulmonary sequelae of COVID-19 is not well known. Purpose: To characterize patterns and rates of improvement of chest CT abnormalities 1 year after COVID-19 pneumonia. Materials and Methods: This was a secondary analysis of a prospective, multicenter observational cohort study conducted from April 29 to August 12, 2020, to assess pulmonary abnormalities at chest CT approximately 2, 3, and 6 months and 1 year after onset of COVID-19 symptoms. Pulmonary findings were graded for each lung lobe using a qualitative CT severity score (CTSS) ranging from 0 (normal) to 25 (all lobes involved). The association of demographic and clinical factors with CT abnormalities after 1 year was assessed with logistic regression. The rate of change of the CTSS at follow-up CT was investigated by using the Friedmann test. Results: Of 142 enrolled participants, 91 underwent a 1-year follow-up CT examination and were included in the analysis (mean age, 59 years ± 13 [SD]; 35 women [38%]). In 49 of 91 (54%) participants, CT abnormalities were observed: 31 of 91 (34%) participants showed subtle subpleural reticulation, ground-glass opacities, or both, and 18 of 91 (20%) participants had extensive ground-glass opacities, reticulations, bronchial dilation, microcystic changes, or a combination thereof. At multivariable analysis, age of more than 60 years (odds ratio [OR], 5.8; 95% CI: 1.7, 24; P = .009), critical COVID-19 severity (OR, 29; 95% CI: 4.8, 280; P < .001), and male sex (OR, 8.9; 95% CI: 2.6, 36; P < .001) were associated with persistent CT abnormalities at 1-year follow-up. Reduction of CTSS was observed in participants at subsequent follow-up CT (P < .001); during the study period, 49% (69 of 142) of participants had complete resolution of CT abnormalities. Thirty-one of 49 (63%) participants with CT abnormalities showed no further improvement after 6 months. Conclusion: Long-term CT abnormalities were common 1 year after COVID-19 pneumonia.},
author = {Luger, Anna K. and Sonnweber, Thomas and Gruber, Leonhard and Schwabl, Christoph and Cima, Katharina and Tymoszuk, Piotr and Gerstner, Anna K. and Pizzini, Alex and Sahanic, Sabina and Boehm, Anna and Coen, Maximilian and Strolz, Carola J. and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Kirchmair, Rudolf and Feuchtner, Gudrun M. and Prosch, Helmut and Tancevski, Ivan and L{\"{o}}ffler-Ragg, Judith and Widmann, Gerlig},
doi = {10.1148/radiol.211670},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luger et al. - 2022 - Chest CT of Lung Injury 1 Year after COVID-19 Pneumonia The CovILD Study.pdf:pdf},
issn = {15271315},
journal = {Radiology},
keywords = {Anna K Luger,Gerlig Widmann,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Thomas Sonnweber,doi:10.1148/radiol.211670,pmid:35348379},
month = {mar},
number = {2},
pages = {462--470},
pmid = {35348379},
publisher = {Radiology},
title = {{Chest CT of Lung Injury 1 Year after COVID-19 Pneumonia: The CovILD Study}},
url = {https://pubmed.ncbi.nlm.nih.gov/35348379/},
volume = {304},
year = {2022}
}
@article{Hufner2022,
abstract = {Background: Coronavirus Disease-19 (COVID-19) convalescents are at risk of developing a de novo mental health disorder or worsening of a pre-existing one. COVID-19 outpatients have been less well characterized than their hospitalized counterparts. The objectives of our study were to identify indicators for poor mental health following COVID-19 outpatient management and to identify high-risk individuals. Methods: We conducted a binational online survey study with adult non-hospitalized COVID-19 convalescents (Austria/AT: n = 1,157, Italy/IT: n = 893). Primary endpoints were positive screening for depression and anxiety (Patient Health Questionnaire; PHQ-4) and self-perceived overall mental health (OMH) and quality of life (QoL) rated with 4 point Likert scales. Psychosocial stress was surveyed with a modified PHQ stress module. Associations of the mental health and QoL with socio-demographic, COVID-19 course, and recovery variables were assessed by multi-parameter Random Forest and Poisson modeling. Mental health risk subsets were defined by self-organizing maps (SOMs) and hierarchical clustering algorithms. The survey analyses are publicly available (https://im2-ibk.shinyapps.io/mental_health_dashboard/). Results: Depression and/or anxiety before infection was reported by 4.6% (IT)/6% (AT) of participants. At a median of 79 days (AT)/96 days (IT) post-COVID-19 onset, 12.4% (AT)/19.3% (IT) of subjects were screened positive for anxiety and 17.3% (AT)/23.2% (IT) for depression. Over one-fifth of the respondents rated their OMH (AT: 21.8%, IT: 24.1%) or QoL (AT: 20.3%, IT: 25.9%) as fair or poor. Psychosocial stress, physical performance loss, high numbers of acute and sub-acute COVID-19 complaints, and the presence of acute and sub-acute neurocognitive symptoms (impaired concentration, confusion, and forgetfulness) were the strongest correlates of deteriorating mental health and poor QoL. In clustering analysis, these variables defined subsets with a particularly high propensity of post-COVID-19 mental health impairment and decreased QoL. Pre-existing depression or anxiety (DA) was associated with an increased symptom burden during acute COVID-19 and recovery. Conclusion: Our study revealed a bidirectional relationship between COVID-19 symptoms and mental health. We put forward specific acute symptoms of the disease as “red flags” of mental health deterioration, which should prompt general practitioners to identify non-hospitalized COVID-19 patients who may benefit from early psychological and psychiatric intervention. Clinical Trial Registration: [ClinicalTrials.gov], identifier [NCT04661462].},
author = {H{\"{u}}fner, Katharina and Tymoszuk, Piotr and Ausserhofer, Dietmar and Sahanic, Sabina and Pizzini, Alex and Rass, Verena and Galffy, Matyas and B{\"{o}}hm, Anna and Kurz, Katharina and Sonnweber, Thomas and Tancevski, Ivan and Kiechl, Stefan and Huber, Andreas and Plagg, Barbara and Wiedermann, Christian J. and Bellmann-Weiler, Rosa and Bachler, Herbert and Weiss, G{\"{u}}nter and Piccoliori, Giuliano and Helbok, Raimund and Loeffler-Ragg, Judith and Sperner-Unterweger, Barbara},
doi = {10.3389/fmed.2022.792881},
issn = {2296858X},
journal = {Frontiers in Medicine},
keywords = {COVID-19,SARS-CoV-2,anxiety,depression,long COVID,machine learning,mental stress,neurocognitive},
month = {mar},
pmid = {35360744},
publisher = {Front Med (Lausanne)},
title = {{Who Is at Risk of Poor Mental Health Following Coronavirus Disease-19 Outpatient Management?}},
url = {https://pubmed.ncbi.nlm.nih.gov/35360744/},
volume = {9},
year = {2022}
}
@article{Sonnweber2022,
abstract = {Background: The optimal procedures to prevent, identify, monitor, and treat long-term pulmonary sequelae of COVID-19 are elusive. Here, we characterized the kinetics of respiratory and symptom recovery following COVID-19. Methods: We conducted a longitudinal, multicenter observational study in ambulatory and hospitalized COVID-19 patients recruited in early 2020 (n = 145). Pulmonary computed tomography (CT) and lung function (LF) readouts, symptom prevalence, and clinical and laboratory parameters were collected during acute COVID-19 and at 60, 100, and 180 days follow-up visits. Recovery kinetics and risk factors were investigated by logistic regression. Classification of clinical features and participants was accomplished by unsupervised and semi-supervised multiparameter clustering and machine learning. Results: At the 6-month follow-up, 49% of participants reported persistent symptoms. The frequency of structural lung CT abnormalities ranged from 18% in the mild outpatient cases to 76% in the intensive care unit (ICU) convalescents. Prevalence of impaired LF ranged from 14% in the mild outpatient cases to 50% in the ICU survivors. Incomplete radiological lung recovery was associated with increased anti-S1/S2 antibody titer, IL-6, and CRP levels at the early follow-up. We demonstrated that the risk of perturbed pulmonary recovery could be robustly estimated at early follow-up by clustering and machine learning classifiers employing solely non-CT and non-LF parameters. Conclusions: The severity of acute COVID-19 and protracted systemic inflammation is strongly linked to persistent structural and functional lung abnormality. Automated screening of multiparameter health record data may assist in the prediction of incomplete pulmonary recovery and optimize COVID-19 follow-up management. Funding: The State of Tyrol (GZ 71934), Boehringer Ingelheim/Investigator initiated study (IIS 1199-0424).},
author = {Sonnweber, Thomas and Tymoszuk, Piotr and Sahanic, Sabina and Boehm, Anna and Pizzini, Alex and Luger, Anna and Schwabl, Christoph and Nairz, Manfred and Grubwieser, Philipp and Kurz, Katharina and Koppelst{\"{a}}tter, Sabine and Aichner, Magdalena and Puchner, Bernhard and Egger, Alexander and Hoermann, Gregor and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Widmann, Gerlig and Tancevski, Ivan and L{\"{o}}ffler-Ragg, Judith},
doi = {10.7554/ELIFE.72500},
issn = {2050084X},
journal = {eLife},
keywords = {Judith L{\"{o}}ffler-Ragg,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Piotr Tymoszuk,PubMed Abstract,Thomas Sonnweber,doi:10.7554/eLife.72500,pmid:35131031},
month = {feb},
pmid = {35131031},
publisher = {Elife},
title = {{Investigating phenotypes of pulmonary COVID-19 recovery: A longitudinal observational prospective multicenter trial}},
url = {https://pubmed.ncbi.nlm.nih.gov/35131031/},
volume = {11},
year = {2022}
}
@article{Wood2017,
abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models. The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book's R data package gamair, to enable use as a course text or for self-study.},
author = {Wood, Simon N.},
doi = {10.1201/9781315370279/GENERALIZED-ADDITIVE-MODELS-SIMON-WOOD},
isbn = {9781498728348},
journal = {Generalized Additive Models: An Introduction with R, Second Edition},
month = {jan},
pages = {1--476},
publisher = {CRC Press},
title = {{Generalized additive models: An introduction with R, second edition}},
url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315370279/generalized-additive-models-simon-wood},
year = {2017}
}
@article{Fasiolo2020,
abstract = {We propose a novel framework for fitting additive quantile regression models, which provides well-calibrated inference about the conditional quantiles and fast automatic estimation of the smoothing...},
archivePrefix = {arXiv},
arxivId = {1707.03307},
author = {Fasiolo, Matteo and Wood, Simon N. and Zaffran, Margaux and Nedellec, Rapha{\"{e}}l and Goude, Yannig},
doi = {10.1080/01621459.2020.1725521},
eprint = {1707.03307},
issn = {1537274X},
journal = {https://doi.org/10.1080/01621459.2020.1725521},
keywords = {Calibrated Bayes,Electricity load forecasting,Generalized additive models,Penalized regression splines,Quantile regression},
number = {535},
pages = {1402--1412},
publisher = {Taylor & Francis},
title = {{Fast Calibrated Additive Quantile Regression}},
url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1725521},
volume = {116},
year = {2020}
}
@article{Wright2017,
abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
archivePrefix = {arXiv},
arxivId = {1508.04409},
author = {Wright, Marvin N. and Ziegler, Andreas},
doi = {10.18637/JSS.V077.I01},
eprint = {1508.04409},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright, Ziegler - 2017 - ranger A Fast Implementation of Random Forests for High Dimensional Data in C and R.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {R,Rcpp,classification,machine learning,random forests,recursive partitioning,survival analysis},
month = {mar},
number = {1},
pages = {1--17},
publisher = {American Statistical Association},
title = {{ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v077i01},
volume = {77},
year = {2017}
}
@misc{Deane-Mayer2019,
abstract = {Description Functions for creating ensembles of caret models: caretList() and caretStack(). caretList() is a convenience function for fitting multiple caret::train() models to the same dataset. caretStack() will make linear or non-linear combinations of these models, using a caret::train() model as a meta-model, and caretEnsemble() will make a robust linear combination of models using a GLM. Depends R (>= 3.2.0)},
author = {Deane-Mayer, Zachary A and Knowles, Jared E},
booktitle = {R package version 2.0.1},
keywords = {MASS,Suggests caTools,caret License MIT + file LICENSE VignetteBuilder k,datatable,digest,e1071,gbm,ggplot2,glmnet,gridExtra,ipred,kernlab,klaR,knitr,lattice,lintr,mlbench,nnet,pROC,pbapply,plyr,randomForest,rmarkdown Imports methods,rpart,testthat},
month = {dec},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{caretEnsemble: Ensembles of Caret Models}},
url = {https://cran.r-project.org/package=caretEnsemble},
year = {2019}
}
@inproceedings{Weston1998,
author = {Weston, Jason and Watkins, Chris},
title = {{Multi-Class Support Vector Machines}},
year = {1998}
}
@article{Kuhn2008,
abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
author = {Kuhn, Max},
doi = {10.18637/jss.v028.i05},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Model building,NetWorkSpaces,Parallel processing,R,Tuning parameters},
number = {5},
pages = {1--26},
publisher = {American Statistical Association},
title = {{Building predictive models in R using the caret package}},
volume = {28},
year = {2008}
}
@book{Ripley2014,
abstract = {Ripley brings together two crucial ideas in pattern recognition: statistical methods and machine learning via neural networks. He brings unifying principles to the fore, and reviews the state of the subject. Ripley also includes many examples to illustrate real problems in pattern recognition and how to overcome them.},
author = {Ripley, Brian D.},
booktitle = {Pattern Recognition and Neural Networks},
doi = {10.1017/CBO9780511812651},
isbn = {9780511812651},
month = {jan},
pages = {1--403},
publisher = {Cambridge University Press},
title = {{Pattern recognition and neural networks}},
url = {https://www.cambridge.org/core/books/pattern-recognition-and-neural-networks/4E038249C9BAA06C8F4EE6F044D09C5C},
year = {2014}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random forests.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
month = {oct},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
url = {https://link.springer.com/article/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@article{Sachs2017,
abstract = {Plots of the receiver operating characteristic (ROC) curve are ubiquitous in medical research. Designed to simultaneously display the operating characteristics at every possible value of a continuous diagnostic test, ROC curves are used in oncology to evaluate screening, diagnostic, prognostic and predictive biomarkers. I reviewed a sample of ROC curve plots from the major oncology journals in order to assess current trends in usage and design elements. My review suggests that ROC curve plots are often ineffective as statistical charts and that poor design obscures the relevant information the chart is intended to display. I describe my new R package that was created to address the shortcomings of existing tools. The package has functions to create informative ROC curve plots, with sensible defaults and a simple interface, for use in print or as an interactive web-based plot. A web application was developed to reach a broader audience of scientists who do not use R.},
author = {Sachs, Michael C.},
doi = {10.18637/jss.v079.c02},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sachs - 2017 - Plotroc A tool for plotting ROC curves.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Graphics,Interactive,Plots,ROC curves},
month = {aug},
number = {1},
pages = {1--19},
publisher = {American Statistical Association},
title = {{Plotroc: A tool for plotting ROC curves}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v079c02/v79c02.pdf https://www.jstatsoft.org/index.php/jss/article/view/v079c02},
volume = {79},
year = {2017}
}
@article{Lopez-Raton2014,
abstract = {Continuous diagnostic tests are often used for discriminating between healthy and diseased populations. For the clinical application of such tests, it is useful to select a cutpoint or discrimination value c that deffnes positive and negative test results. In general, individuals with a diagnostic test value of c or higher are classiffed as diseased. Several search strategies have been proposed for choosing optimal cutpoints in diagnostic tests, depending on the underlying reason for this choice. This paper introduces an R package, known as OptimalCutpoints, for selecting optimal cutpoints in diagnostic tests. It incorporates criteria that take the costs of the different diagnostic decisions into account, as well as the prevalence of the target disease and several methods based on measures of diagnostic test accuracy. Moreover, it enables optimal levels to be calculated according to levels of given (categorical) covariates. While the numerical output includes the optimal cutpoint values and associated accuracy measures with their conffdence intervals, the graphical output includes the receiver operating characteristic (ROC) and predictive ROC curves. An illustration of the use of OptimalCutpoints is provided, using a real biomedical dataset.},
author = {L{\'{o}}pez-Rat{\'{o}}n, M{\'{o}}nica and Rodr{\'{i}}guez-{\'{A}}lvarez, Mar{\'{i}}a Xos{\'{e}} and Cadarso-Su{\'{a}}rez, Carmen and Gude-Sampedro, Francisco},
doi = {10.18637/jss.v061.i08},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\'{o}}pez-Rat{\'{o}}n et al. - 2014 - Optimalcutpoints An R package for selecting optimal cutpoints in diagnostic tests.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Accuracy measures,Diagnostic tests,Optimal cutpoint,R,ROC curve},
month = {nov},
number = {8},
pages = {1--36},
publisher = {American Statistical Association},
title = {{Optimalcutpoints: An R package for selecting optimal cutpoints in diagnostic tests}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v061i08/v61i08.pdf https://www.jstatsoft.org/index.php/jss/article/view/v061i08},
volume = {61},
year = {2014}
}
@article{Sonnweber2020,
abstract = {Background: After the 2002/2003 severe acute respiratory syndrome outbreak, 30% of survivors exhibited persisting structural pulmonary abnormalities. The long-term pulmonary sequelae of coronavirus disease 2019 (COVID-19) are yet unknown, and comprehensive clinical follow-up data are lacking. Methods: In this prospective, multicentre, observational study, we systematically evaluated the cardiopulmonary damage in subjects recovering from COVID-19 at 60 and 100 days after confirmed diagnosis. We conducted a detailed questionnaire, clinical examination, laboratory testing, lung function analysis, echocardiography and thoracic low-dose computed tomography (CT). Results: Data from 145 COVID-19 patients were evaluated, and 41% of all subjects exhibited persistent symptoms 100 days after COVID-19 onset, with dyspnoea being most frequent (36%). Accordingly, patients still displayed an impaired lung function, with a reduced diffusing capacity in 21% of the cohort being the most prominent finding. Cardiac impairment, including a reduced left ventricular function or signs of pulmonary hypertension, was only present in a minority of subjects. CT scans unveiled persisting lung pathologies in 63% of patients, mainly consisting of bilateral ground-glass opacities and/or reticulation in the lower lung lobes, without radiological signs of pulmonary fibrosis. Sequential follow-up evaluations at 60 and 100 days after COVID-19 onset demonstrated a vast improvement of symptoms and CT abnormalities over time. Conclusion: A relevant percentage of post-COVID-19 patients presented with persisting symptoms and lung function impairment along with radiological pulmonary abnormalities >100 days after the diagnosis of COVID-19. However, our results indicate a significant improvement in symptoms and cardiopulmonary status over time.},
author = {Sonnweber, Thomas and Sahanic, Sabina and Pizzini, Alex and Luger, Anna and Schwabl, Christoph and Sonnweber, Bettina and Kurz, Katharina and Koppelst{\"{a}}tter, Sabine and Haschka, David and Petzer, Verena and Boehm, Anna and Aichner, Magdalena and Tymoszuk, Piotr and Lener, Daniela and Theurl, Markus and Lorsbach-K{\"{o}}hler, Almut and Tancevski, Amra and Schapfl, Anna and Schaber, Marc and Hilbe, Richard and Nairz, Manfred and Puchner, Bernhard and H{\"{u}}ttenberger, Doris and Tschurtschenthaler, Christoph and A{\ss}hoff, Malte and Peer, Andreas and Hartig, Frank and Bellmann, Romuald and Joannidis, Michael and Gollmann-Tepek{\"{o}}yl{\"{u}}, Can and Holfeld, Johannes and Feuchtner, Gudrun and Egger, Alexander and Hoermann, Gregor and Schroll, Andrea and Fritsche, Gernot and Wildner, Sophie and Bellmann-Weiler, Rosa and Kirchmair, Rudolf and Helbok, Raimund and Prosch, Helmut and Rieder, Dietmar and Trajanoski, Zlatko and Kronenberg, Florian and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Widmann, Gerlig and L{\"{o}}ffler-Ragg, Judith and Tancevski, Ivan},
doi = {10.1183/13993003.03481-2020},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sonnweber et al. - 2020 - Cardiopulmonary recovery after COVID-19 - an observational prospective multi-center trial.pdf:pdf},
issn = {13993003},
journal = {European Respiratory Journal},
keywords = {Ivan Tancevski,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC7736754,PubMed Abstract,Sabina Sahanic,Thomas Sonnweber,doi:10.1183/13993003.03481-2020,pmid:33303539},
month = {dec},
number = {4},
pmid = {33303539},
publisher = {Eur Respir J},
title = {{Cardiopulmonary recovery after COVID-19: An observational prospective multicentre trial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/33303539},
volume = {57},
year = {2021}
}
@incollection{Venables2002,
author = {Venables, Bill and Ripley, B},
booktitle = {Springer},
doi = {10.1007/b97626},
title = {{Modern Applied Statistics With S}},
year = {2002}
}
@book{Xie2016,
abstract = {bookdown: Authoring Books and Technical Documents with R Markdown presents a much easier way to write books and technical publications than traditional tools such as LaTeX and Word. The bookdown package inherits the simplicity of syntax and flexibility for data analysis from R Markdown, and extends R Markdown for technical writing, so that you can make better use of document elements such as figures, tables, equations, theorems, citations, and references. Similar to LaTeX, you can number and cross-reference these elements with bookdown. Your document can even include live examples so readers can interact with them while reading the book. The book can be rendered to multiple output formats, including LaTeX/PDF, HTML, EPUB, and Word, thus making it easy to put your documents online. The style and theme of these output formats can be customized. We used books and R primarily for examples in this book, but bookdown is not only for books or R. Most features introduced in this book also apply to other types of publications: journal papers, reports, dissertations, course handouts, study notes, and even novels. You do not have to use R, either. Other choices of computing languages include Python, C, C plus plus, SQL, Bash, Stan, JavaScript, and so on, although R is best supported. You can also leave out computing, for example, to write a fiction. This book itself is an example of publishing with bookdown and R Markdown, and its source is fully available on GitHub.},
author = {Xie, Yihui},
booktitle = {Bookdown: Authoring Books and Technical Documents with R Markdown},
doi = {10.1201/9781315204963},
isbn = {9781351792608},
pages = {1--113},
title = {{Bookdown: Authoring books and technical documents with R Markdown}},
year = {2016}
}
@article{Benjamini1995,
abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses- the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
author = {Benjamini, Yoav and Hochberg, Yosef},
doi = {10.1111/j.2517-6161.1995.tb02031.x},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {bonferroni‐type procedures,familywise error rate,multiple‐comparison procedures,p‐values},
month = {jan},
number = {1},
pages = {289--300},
publisher = {Wiley},
title = {{Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing}},
volume = {57},
year = {1995}
}
@book{Hastie1991,
abstract = {First edition. Scope and content: "Statistical Models in S extends the S language to fit and analyze a variety of statistical models, including analysis of variance, generalized linear models, additive models, local regression, and tree-based models. The contributions of the ten authors-most of whom work in the statistics research department at AT & T Bell Laboratories-represent results of research in both the computational and statistical aspects of modeling data."--Provided by publisher. Chapter 1 An Appetizer -- chapter 2 Statistical Models -- chapter 3 Data for Models -- chapter 4 Linear Models -- chapter 5 Analysis of Variance; Designed Experiments -- chapter 6 Generalized Linear Models -- chapter 7 Generalized Additive Models -- chapter 8 Local Regression Models -- chapter 9 Tree-Based Models -- chapter 10 Nonlinear Models.},
address = {London},
author = {Hastie, T. J.},
edition = {1},
isbn = {1351414224},
publisher = {Routledge},
title = {{Statistical Models in S.}},
year = {1991}
}
@book{Wilke2019,
abstract = {First edition. Intro; Copyright; Table of Contents; Preface; Thoughts on Graphing Software and Figure-Preparation Pipelines; Conventions Used in This Book; Using Code Examples; O'Reilly Online Learning; How to Contact Us; Acknowledgments; Chapter 1. Introduction; Ugly, Bad, and Wrong Figures; Part I. From Data to Visualization; Chapter 2. Visualizing Data: Mapping Data onto Aesthetics; Aesthetics and Types of Data; Scales Map Data Values onto Aesthetics; Chapter 3. Coordinate Systems and Axes; Cartesian Coordinates; Nonlinear Axes; Coordinate Systems with Curved Axes; Chapter 4. Color Scales Color as a Tool to DistinguishColor to Represent Data Values; Color as a Tool to Highlight; Chapter 5. Directory of Visualizations; Amounts; Distributions; Proportions; x-y relationships; Geospatial Data; Uncertainty; Chapter 6. Visualizing Amounts; Bar Plots; Grouped and Stacked Bars; Dot Plots and Heatmaps; Chapter 7. Visualizing Distributions: Histograms and Density Plots; Visualizing a Single Distribution; Visualizing Multiple Distributions at the Same Time; Chapter 8. Visualizing Distributions: Empirical Cumulative Distribution Functions and Q-Q Plots Empirical Cumulative Distribution FunctionsHighly Skewed Distributions; Quantile-Quantile Plots; Chapter 9. Visualizing Many Distributions at Once; Visualizing Distributions Along the Vertical Axis; Visualizing Distributions Along the Horizontal Axis; Chapter 10. Visualizing Proportions; A Case for Pie Charts; A Case for Side-by-Side Bars; A Case for Stacked Bars and Stacked Densities; Visualizing Proportions Separately as Parts of the Total; Chapter 11. Visualizing Nested Proportions; Nested Proportions Gone Wrong; Mosaic Plots and Treemaps; Nested Pies; Parallel Sets Chapter 12. Visualizing Associations Among Two or More Quantitative VariablesScatterplots; Correlograms; Dimension Reduction; Paired Data; Chapter 13. Visualizing Time Series and Other Functions of an Independent Variable; Individual Time Series; Multiple Time Series and Dose-Response Curves; Time Series of Two or More Response Variables; Chapter 14. Visualizing Trends; Smoothing; Showing Trends with a Defined Functional Form; Detrending and Time-Series Decomposition; Chapter 15. Visualizing Geospatial Data; Projections; Layers; Choropleth Mapping; Cartograms Chapter 16. Visualizing UncertaintyFraming Probabilities as Frequencies; Visualizing the Uncertainty of Point Estimates; Visualizing the Uncertainty of Curve Fits; Hypothetical Outcome Plots; Part II. Principles of Figure Design; Chapter 17. The Principle of Proportional Ink; Visualizations Along Linear Axes; Visualizations Along Logarithmic Axes; Direct Area Visualizations; Chapter 18. Handling Overlapping Points; Partial Transparency and Jittering; 2D Histograms; Contour Lines; Chapter 19. Common Pitfalls of Color Use; Encoding Too Much or Irrelevant Information},
address = {Sebastopol},
author = {Wilke, Claus O},
booktitle = {O'Reilly Media},
edition = {1},
isbn = {1492031089},
pages = {389},
publisher = {O'Reilly Media},
title = {{Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures}},
year = {2019}
}
@article{Wickham2019,
abstract = {At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. The tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming. We expect that almost every project will use multiple domain-specific packages outside of the tidyverse: our goal is to provide tooling for the most common challenges; not to solve every possible problem. Notably, the tidyverse doesn't include tools for statistical modelling or communication. These toolkits are critical for data science, but are so large that they merit separate treatment. The tidyverse package allows users to install all tidyverse packages with a single command. There are a number of projects that are similar in scope to the tidyverse. The closest is perhaps Bioconductor (Gentleman et al., 2004; Huber et al., 2015), which provides an ecosystem of packages that support the analysis of high-throughput genomic data.},
author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c{c}}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"{u}}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
doi = {10.21105/joss.01686},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf:pdf},
issn = {2475-9066},
journal = {Journal of Open Source Software},
month = {nov},
number = {43},
pages = {1686},
publisher = {The Open Journal},
title = {{Welcome to the Tidyverse}},
volume = {4},
year = {2019}
}
@book{Wickham2016,
address = {New York},
author = {Wickham, Hadley.},
edition = {1},
isbn = {978-3-319-24277-4},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {https://ggplot2.tidyverse.org},
year = {2016}
}
@article{Boulogne2024,
abstract = {Background: Automated estimation of Pulmonary function test (PFT) results from Computed Tomography (CT) could advance the use of CT in screening, diagnosis, and staging of restrictive pulmonary diseases. Estimating lung function per lobe, which cannot be done with PFTs, would be helpful for risk assessment for pulmonary resection surgery and bronchoscopic lung volume reduction. Purpose: To automatically estimate PFT results from CT and furthermore disentangle the individual contribution of pulmonary lobes to a patient's lung function. Methods: We propose I3Dr, a deep learning architecture for estimating global measures from an image that can also estimate the contributions of individual parts of the image to this global measure. We apply it to estimate the separate contributions of each pulmonary lobe to a patient's total lung function from CT, while requiring only CT scans and patient level lung function measurements for training. I3Dr consists of a lobe-level and a patient-level model. The lobe-level model extracts all anatomical pulmonary lobes from a CT scan and processes them in parallel to produce lobe level lung function estimates that sum up to a patient level estimate. The patient-level model directly estimates patient level lung function from a CT scan and is used to re-scale the output of the lobe-level model to increase performance. After demonstrating the viability of the proposed approach, the I3Dr model is trained and evaluated for PFT result estimation using a large data set of 8 433 CT volumes for training, 1 775 CT volumes for validation, and 1 873 CT volumes for testing. Results: First, we demonstrate the viability of our approach by showing that a model trained with a collection of digit images to estimate their sum implicitly learns to assign correct values to individual digits. Next, we show that our models can estimate lobe-level quantities, such as COVID-19 severity scores, pulmonary volume (PV), and functional pulmonary volume (FPV) from CT while only provided with patient-level quantities during training. Lastly, we train and evaluate models for producing spirometry and diffusion capacity of carbon mono-oxide (DLCO) estimates at the patient and lobe level. For producing Forced Expiratory Volume in one second (FEV1), Forced Vital Capacity (FVC), and DLCO estimates, I3Dr obtains mean absolute errors (MAE) of 0.377 L, 0.297 L, and 2.800 mL/min/mm Hg respectively. We release the resulting algorithms for lung function estimation to the research community at https://grand-challenge.org/algorithms/lobe-wise-lung-function-estimation/. Conclusions: I3Dr can estimate global measures from an image, as well as the contributions of individual parts of the image to this global measure. It offers a promising approach for estimating PFT results from CT scans and disentangling the individual contribution of pulmonary lobes to a patient's lung function. The findings presented in this work may advance the use of CT in screening, diagnosis, and staging of restrictive pulmonary diseases as well as in risk assessment for pulmonary resection surgery and bronchoscopic lung volume reduction.},
author = {Boulogne, Luuk H. and Charbonnier, Jean Paul and Jacobs, Colin and van der Heijden, Erik H.F.M. and van Ginneken, Bram},
doi = {10.1002/MP.16915},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boulogne et al. - 2024 - Estimating lung function from computed tomography at the patient and lobe level using machine learning.pdf:pdf},
issn = {2473-4209},
journal = {Medical Physics},
keywords = {computed tomography,convolutional neural network,pulmonary function test,weakly supervised learning},
month = {apr},
number = {4},
pages = {2834--2845},
pmid = {38329315},
publisher = {John Wiley & Sons, Ltd},
title = {{Estimating lung function from computed tomography at the patient and lobe level using machine learning}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/mp.16915 https://onlinelibrary.wiley.com/doi/abs/10.1002/mp.16915 https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16915},
volume = {51},
year = {2024}
}
@article{McHugh2012,
abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
author = {McHugh, Mary L.},
doi = {10.11613/bm.2012.031},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McHugh - 2012 - Interrater reliability the kappa statistic(2).pdf:pdf},
issn = {13300962},
journal = {Biochemia Medica},
keywords = {Interrater,Kappa,Rater,Reliability},
number = {3},
pages = {276},
pmid = {23092060},
publisher = {Croatian Society for Medical Biochemistry and Laboratory Medicine},
title = {{Interrater reliability: the kappa statistic}},
url = {/pmc/articles/PMC3900052/ /pmc/articles/PMC3900052/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/},
volume = {22},
year = {2012}
}
